{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Nyarch Linux Smart Prompts trainer\n",
        "\n",
        "## Settings"
      ],
      "metadata": {
        "id": "0TpDNau6IQW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VERSION = 0.2\n",
        "EMBEDDINGS_SIZE = 1024\n",
        "NAME = \"NyaMedium\"\n",
        "FILENAME = NAME + '_' + str(VERSION) + '.pkl'\n",
        "DATASET_URL = 'https://raw.githubusercontent.com/NyarchLinux/Smart-Prompts/refs/heads/main/dataset.csv'"
      ],
      "metadata": {
        "id": "in6BcROV9lUj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Management"
      ],
      "metadata": {
        "id": "YVavfAHYIYb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def save_dataset_to_csv(dataset, filename):\n",
        "  \"\"\"Saves a dataset dictionary to a CSV file.\n",
        "\n",
        "  Args:\n",
        "    dataset: A dictionary where keys are labels and values are lists of prompts.\n",
        "    filename: The name of the CSV file to save.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Prompt', 'Label'])  # Header row\n",
        "\n",
        "    for label, prompts in dataset.items():\n",
        "      for prompt in prompts:\n",
        "        writer.writerow([prompt, label])"
      ],
      "metadata": {
        "id": "HhcuBgnTzq09"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def reconstruct_dataset_from_csv(filename):\n",
        "  \"\"\"Reconstructs a dataset dictionary from a CSV file.\n",
        "\n",
        "  Args:\n",
        "    filename: The name of the CSV file to load.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary where keys are labels and values are lists of prompts.\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "  with open(filename, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    next(reader)  # Skip the header row\n",
        "\n",
        "    for row in reader:\n",
        "      prompt, label = row\n",
        "      if label not in dataset:\n",
        "        dataset[label] = []\n",
        "      dataset[label].append(prompt)\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "XiVMvChqHIRT"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional downloads"
      ],
      "metadata": {
        "id": "C7JqfhM5Ii6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U wordllama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzQEvfmMqToj",
        "outputId": "436af14f-baf6-4e4d-a5f8-18fc0fc8f4c7",
        "collapsed": true
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordllama in /usr/local/lib/python3.10/dist-packages (0.3.3.post0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from wordllama) (1.26.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from wordllama) (0.4.5)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from wordllama) (0.19.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from wordllama) (0.10.2)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from wordllama) (2.9.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wordllama) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->wordllama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->wordllama) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->wordllama) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wordllama) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wordllama) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wordllama) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wordllama) (2024.8.30)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->wordllama) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset"
      ],
      "metadata": {
        "id": "faHVW7ybIqM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.check_output([\"wget\", \"-O\", \"dataset.csv\", DATASET_URL])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gh8W7RBIsWt",
        "outputId": "75e80e32-e204-4444-abfb-1a472e6e03a0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b''"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = reconstruct_dataset_from_csv('dataset.csv')"
      ],
      "metadata": {
        "id": "gwVxKghLI8Im"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation and split"
      ],
      "metadata": {
        "id": "Yd9XVMUuCryC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordllama import WordLlama\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Conver the dataset into a label list\n",
        "texts = []\n",
        "labels = []\n",
        "for label, questions in DATASET.items():\n",
        "    texts.extend(questions)\n",
        "    labels.extend([label] * len(questions))\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Start the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "wl = WordLlama.load(dim=EMBEDDINGS_SIZE)\n",
        "\n",
        "# Embend all the prompts\n",
        "X_train_embeddings = wl.embed(X_train)\n",
        "X_test_embeddings = wl.embed(X_test)\n"
      ],
      "metadata": {
        "id": "5-1weNCjzsHv"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "dJIVtmSiCufM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Create an SVC classifier\n",
        "classifier = LogisticRegression(random_state=22)\n",
        "\n",
        "# Train the model\n",
        "classifier.fit(X_train_embeddings, y_train_encoded)\n",
        "\n",
        "# Predict the test dataset\n",
        "y_pred = classifier.predict(X_test_embeddings)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyA-_yehzzNE",
        "outputId": "d8ffc480-a3e0-48c6-807f-ad0ce3fa5743"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      codecs       1.00      1.00      1.00        19\n",
            "  colloquial       0.97      0.97      0.97        35\n",
            "     console       0.97      0.97      0.97        30\n",
            "      docker       0.96      1.00      0.98        23\n",
            "      nvidia       1.00      1.00      1.00        47\n",
            "      ollama       1.00      0.88      0.93        16\n",
            "       table       0.97      1.00      0.98        28\n",
            "    voicevox       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           0.98       209\n",
            "   macro avg       0.98      0.98      0.98       209\n",
            "weighted avg       0.98      0.98      0.98       209\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export the model and use it"
      ],
      "metadata": {
        "id": "8KaUCOo9CycE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "\n",
        "import pickle\n",
        "\n",
        "pickle.dump(classifier, open(FILENAME, 'wb'))\n"
      ],
      "metadata": {
        "id": "hpOmjrO7Ptsp"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and test the model"
      ],
      "metadata": {
        "id": "96xs5TN2C2rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from wordllama import WordLlama\n",
        "\n",
        "# Load the model\n",
        "loaded_model = pickle.load(open(FILENAME, 'rb'))\n",
        "\n",
        "# Load wordllama\n",
        "wl = WordLlama.load(dim=EMBEDDINGS_SIZE)"
      ],
      "metadata": {
        "id": "W6rY8hFjnKXO"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences to classify\n",
        "new_texts = [\"How do I install proprietary nvidia drivers\"]\n",
        "\n",
        "# Embed new sentences\n",
        "new_embeddings = wl.embed(new_texts)\n",
        "\n",
        "# Get the probabilities\n",
        "new_probabilities = classifier.predict_proba(new_embeddings)\n",
        "\n",
        "# Print the probabilities\n",
        "labels = list(DATASET.keys())\n",
        "labels.sort()\n",
        "for i, text in enumerate(new_texts):\n",
        "    print(f\"Text: '{text}'\")\n",
        "    for j, category in enumerate(labels):\n",
        "        print(f\"  {category}: {new_probabilities[i][j]:.4f}\")\n",
        "    print(\"Category:\" + label_encoder.classes_[np.argmax(new_probabilities[i])])\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCWZoDcg1Nr_",
        "outputId": "5f480c48-0b67-45be-9920-8983de4069f2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'How do I install proprietary nvidia drivers'\n",
            "  codecs: 0.0005\n",
            "  colloquial: 0.0000\n",
            "  console: 0.0006\n",
            "  docker: 0.0001\n",
            "  nvidia: 0.9983\n",
            "  ollama: 0.0001\n",
            "  table: 0.0000\n",
            "  voicevox: 0.0004\n",
            "Category:nvidia\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}